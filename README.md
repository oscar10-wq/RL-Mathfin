# Portfolio Selection using Reinforcement Learning 

In this project, I am working on a reinforcement learning algorithm that solves the exploratory mean-variance problem. This algorithm uses accumulative entropy to create exploration in portfolio strategies to achieve a specified target return. The advantage of using such an algorithm over more famous methods such as deep deterministic policy gradient or maximum likelihood estimation is the shorter training time and the lower sensitivity to parameter tuning. This work is inspired by the research paper "Continuous-time mean-variance portfolio selection: A reinforcement learning framework" by Haoran Wang and Xhu Yu Zhou.  


This entire project is based on two important results which are the following: 




### **Theorem 4.0** (Policy Improvement Theorem) [Wang & Zhou, 2020]

Let `w âˆˆ â„`, `i âˆˆ â„•`, and let `fâ± = fâ±(Â·; Â·, Â·, w)` be an admissible feedback control.  
Assume the corresponding value function `J^{fâ±}(Â·, Â·; w)` lies in  
`C^{1,2}([0,T) Ã— â„) âˆ© Câ°([0,T] Ã— â„)`  
and satisfies `J_{vv}^{fâ±}(t, v; w) > 0` for all `(t, v) âˆˆ [0, T) Ã— â„`.

Suppose the feedback control `fâ±âºÂ¹` is defined by:

```
fâ±âºÂ¹(Î¸; t, v, w) = f_{ğ’©(Î¼, ÏƒÂ²)}(Î¸)

where:
  Î¼   = - (Î³ / Ïƒ) * (J_v^{fâ±}(t, v; w)) / (J_{vv}^{fâ±}(t, v; w))
  ÏƒÂ²  = Î» / (ÏƒÂ² * J_{vv}^{fâ±}(t, v; w))
```

and `fâ±âºÂ¹` is admissible. Then the value function improves (or remains the same):

```
J^{fâ±âºÂ¹}(t, v; w) â‰¤ J^{fâ±}(t, v; w)   for all (t, v) âˆˆ [0, T] Ã— â„
```
